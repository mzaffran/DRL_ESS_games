{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook trains a defender agent with PPO\n",
    "\n",
    "This notebook offers example code on how to train a defender agent on the ESS environment with PPO. Note that for the code to work correctly, you'll need the modified versions of gym and OpenAI baselines installed (we recommend on a virtual environment). \n",
    "\n",
    "Links to modified gym/baselines:\n",
    "\n",
    "\n",
    "https://github.com/rubai5/baselines\n",
    "\n",
    "\n",
    "https://github.com/rubai5/gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline \n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from mpi4py import MPI\n",
    "import os.path as osp\n",
    "import gym, logging\n",
    "import baselines\n",
    "from baselines import logger\n",
    "from baselines.ppo1 import pposgd_simple_generalization, mlp_policy\n",
    "import baselines.common.tf_util as U\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Paramters\n",
    "The ESS game has a huge number of possible states. The gym environment has some ways of sampling from these states, and here, we set the parameters to mix the distributions as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "name = \"ErdosGame-v0\"\n",
    "seed = 101\n",
    "\n",
    "# game specific parameters\n",
    "K = 15\n",
    "potential = 0.9\n",
    "\n",
    "# sampling probabilities, must sum to 1\n",
    "unif_prob = 0.0\n",
    "geo_prob = 1.0\n",
    "diverse_prob = 0.0\n",
    "state_unif_prob = 0.0 # can only use if K is small < 10 -- try to use previous methods instead\n",
    "\n",
    "assert (unif_prob + geo_prob + diverse_prob + state_unif_prob == 1), \"probabilites don't sum to 1\"\n",
    "\n",
    "# attacker plays adversarially?\n",
    "adverse_set_prob = 0.0\n",
    "disj_supp_prob = 0.5\n",
    "\n",
    "# high one\n",
    "high_one_prob = 0.0\n",
    "\n",
    "# upper limits for start state sampling\n",
    "geo_high = K - 2\n",
    "unif_high = max(3, K-3)\n",
    "\n",
    "# putting into names_and_args argument\n",
    "names_and_args = {\"K\" : K, \n",
    "                  \"potential\" : potential, \n",
    "                  \"unif_prob\" : unif_prob, \n",
    "                  \"geo_prob\" : geo_prob,\n",
    "                   \"diverse_prob\" : diverse_prob, \n",
    "                  \"state_unif_prob\" : state_unif_prob, \n",
    "                  \"high_one_prob\" : high_one_prob, \n",
    "                  \"adverse_set_prob\" :adverse_set_prob, \n",
    "                  \"disj_supp_prob\" : disj_supp_prob, \n",
    "                  \"geo_high\" : geo_high, \n",
    "                  \"unif_high\" :unif_high }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HID_SIZE=300\n",
    "NUM_HID_LAYERS=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Net, Train and Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to initialize environment and train model\n",
    "\n",
    "def policy_fn(name, ob_space, ac_space):\n",
    "        return mlp_policy.MlpPolicy(name=name, ob_space=ob_space, ac_space=ac_space, \n",
    "                                    hid_size=HID_SIZE, num_hid_layers=NUM_HID_LAYERS)\n",
    "    \n",
    "def make_policies(ob_space, ac_space, policy_func):\n",
    "    pi = policy_func(\"pi\", ob_space, ac_space)\n",
    "    oldpi = policy_func(\"old_pi\", ob_space, ac_space)\n",
    "    return pi, oldpi\n",
    "\n",
    "def train(env_train, pi, oldpi, names_and_args, num_timesteps, test_envs):\n",
    "    #workerseed = seed + 10000 * MPI.COMM_WORLD.Get_rank()\n",
    "    #set_global_seeds(workerseed)\n",
    "    \n",
    "    env_train.reset()\n",
    "    if test_envs:\n",
    "        for test_env in test_envs:\n",
    "            test_env.reset()\n",
    "    \n",
    "    #env.seed(workerseed)\n",
    "    gym.logger.setLevel(logging.WARN)\n",
    "    # create file handler which logs even debug messages\n",
    "    fh = logging.FileHandler('spam.log')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    # create console handler with a higher log level\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.ERROR)\n",
    "    # create formatter and add it to the handlers\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "    # add the handlers to the logger\n",
    "    gym.logger.addHandler(fh)\n",
    "    gym.logger.addHandler(ch)\n",
    "        \n",
    "\n",
    "    policy_net, info = pposgd_simple_generalization.learn(env_train, pi, oldpi,\n",
    "        max_timesteps=num_timesteps,\n",
    "        timesteps_per_batch=100,\n",
    "        clip_param=0.2, entcoeff=0.01,\n",
    "        optim_epochs=4, optim_stepsize=1e-3, optim_batchsize=50,\n",
    "        gamma=0.99, lam=0.95,\n",
    "        schedule='linear',\n",
    "        test_envs=test_envs\n",
    "    )\n",
    "\n",
    "    return policy_net, info\n",
    "\n",
    "\n",
    "def test_policy(num_rounds, policy_net, test_env):\n",
    "    total_reward = 0.0\n",
    "    horizon = test_env.observation_space.K*num_rounds # generate around num_rounds draws\n",
    "    seg_gen = pposgd_simple_generalization.traj_segment_generator(policy_net, test_env, horizon, stochastic=True)\n",
    "    \n",
    "    # call generator\n",
    "    results = seg_gen.__next__()\n",
    "    mean_reward = np.mean(results[\"ep_rets\"])\n",
    "    actions = results[\"ac\"]\n",
    "    labels = results[\"label\"]\n",
    "    mean_correct_actions = compute_correct_actions(labels, actions)\n",
    "    return mean_reward, mean_correct_actions\n",
    "\n",
    "def compute_correct_actions(label, ac):\n",
    "    count = 0\n",
    "    idxs = np.all((label == [1,1]), axis=1)\n",
    "    count += np.sum(idxs)\n",
    "    new_label = label[np.invert(idxs)]\n",
    "    new_ac = ac[np.invert(idxs)]\n",
    "    count += np.sum((new_ac == np.argmax(new_label, axis=1)))\n",
    "    avg = count/len(label)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on sessions\n",
    "To run most of the baselines code, we need to explicitly state that the session is the default one, i.e. start with\n",
    "    <code here>\n",
    "    with sess.as_default():\n",
    "    </code here>\n",
    "The code is currently set up for initializing sess = U.single_threaded_session() as a global variable and closing/reseting the graph explicitly to enable restarts, etc. Note that U.reset() must be used along with tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to load graphs and sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "def reset_session_and_graph():\n",
    "    try:\n",
    "        sess.close()\n",
    "    except:\n",
    "        pass\n",
    "    tf.reset_default_graph()\n",
    "    U.reset()\n",
    "    \n",
    "def save_session(fp):\n",
    "    # saves session\n",
    "    assert fp[-5:] == \".ckpt\", \"checkpoint name must end with .ckpt\"\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, fp)\n",
    "    \n",
    "def load_session_and_graph(fp_meta, fp_ckpt):\n",
    "    saver = tf.train.import_meta_graph(fp_meta)\n",
    "    saver.restore(sess, fp_ckpt)\n",
    "    U.load_state(fp_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0306 21:00:14.135459 10872 registration.py:137] Making new env: ErdosGame-v0\n",
      "[2020-03-06 21:00:14,135] Making new env: ErdosGame-v0\n",
      "D:\\Programmes\\Anaconda\\lib\\site-packages\\gym-0.9.3-py3.7.egg\\gym\\envs\\registration.py:18: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "erdos_env = gym.make(name, **names_and_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erdos_env.reset()\n",
    "# state = erdos_env.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erdos_env.env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erdos_env.env.viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erdos_env.env.state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erdos_env.env.state[:erdos_env.env.K+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erdos_env.env.state[:erdos_env.env.K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erdos_env.env.state[erdos_env.env.K+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erdos_env.env.game_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erdos_env.env.game_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    " \n",
    "# def fig2data ( fig ):\n",
    "#     \"\"\"\n",
    "#     @brief Convert a Matplotlib figure to a 4D numpy array with RGBA channels and return it\n",
    "#     @param fig a matplotlib figure\n",
    "#     @return a numpy 3D array of RGBA values\n",
    "#     \"\"\"\n",
    "#     # draw the renderer\n",
    "#     fig.canvas.draw()\n",
    " \n",
    "#     # Get the RGBA buffer from the figure\n",
    "#     w,h = fig.canvas.get_width_height()\n",
    "#     buf = numpy.frombuffer( fig.canvas.tostring_argb(), dtype=numpy.uint8 )\n",
    "#     buf.shape = ( w, h,4 )\n",
    " \n",
    "#     # canvas.tostring_argb give pixmap in ARGB mode. Roll the ALPHA channel to have it in RGBA mode\n",
    "#     buf = numpy.roll ( buf, 3, axis = 2 )\n",
    "#     return buf\n",
    "\n",
    "# from PIL import Image \n",
    "# def fig2img ( fig ):\n",
    "#     \"\"\"\n",
    "#     @brief Convert a Matplotlib figure to a PIL Image in RGBA format and return it\n",
    "#     @param fig a matplotlib figure\n",
    "#     @return a Python Imaging Library ( PIL ) image\n",
    "#     \"\"\"\n",
    "#     # put the figure pixmap into a numpy array\n",
    "#     buf = fig2data (fig)\n",
    "#     w, h, d = buf.shape\n",
    "#     return Image.frombytes( \"RGBA\", ( w ,h ), buf.tostring( ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 5\n",
    "# A_state = erdos_env.env.state[:erdos_env.env.K+1]\n",
    "# B_state = erdos_env.env.state[erdos_env.env.K+1:]\n",
    "# print(A_state.shape, B_state.shape)\n",
    "# assert A_state.shape == B_state.shape\n",
    "# ind = np.arange(erdos_env.env.K+1)    # the x locations for the groups\n",
    "# width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "# fig = plt.figure()\n",
    "# p1 = plt.bar(ind, A_state, width)\n",
    "# p2 = plt.bar(ind, B_state, width,\n",
    "#              bottom=A_state)\n",
    "# plt.ylabel('Scores')\n",
    "# plt.title('Board state at step: {}'.format(erdos_env.env.steps))\n",
    "# plt.xticks(ind, ['L{}'.format(i) for i in ind])\n",
    "# plt.legend((p1[0], p2[0]), ('A', 'B'))\n",
    "# im = fig2img (fig)\n",
    "# plt.close()\n",
    "# im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-03-06 21:11:02,042] Making new env: ErdosGame-v0\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "reset_session_and_graph()\n",
    "sess = U.single_threaded_session()\n",
    "num_timesteps = 50000\n",
    "pposgd_simple_generalization.logger.set_level(0)\n",
    "with sess.as_default():\n",
    "    # Train agent\n",
    "    env_train =  gym.make(name, **names_and_args)\n",
    "    pi, oldpi = make_policies(erdos_env.observation_space, \n",
    "                              erdos_env.action_space, \n",
    "                              policy_fn)\n",
    "    policy_net, info = pposgd_simple_generalization.learn(\n",
    "        env_train, pi, oldpi,\n",
    "        max_timesteps=num_timesteps,\n",
    "        timesteps_per_batch=100,\n",
    "        clip_param=0.2, entcoeff=0.01,\n",
    "        optim_epochs=4, optim_stepsize=1e-3, optim_batchsize=50,\n",
    "        gamma=0.99, lam=0.95,\n",
    "        schedule='linear',\n",
    "        test_envs=list(),\n",
    "        callback = None\n",
    "    )\n",
    "    \n",
    "    # Test agent\n",
    "    env = gym.make(name, **names_and_args)\n",
    "    pi = policy_net\n",
    "    num_rounds = 2\n",
    "    stochastic = True\n",
    "    horizon = env.observation_space.K * num_rounds\n",
    "    t = 0\n",
    "    ac = env.action_space.sample() # not used, just so we have the datatype\n",
    "    new = True # marks if we're on first timestep of an episode\n",
    "    ob = env.reset()\n",
    "    label = env.env.labels()\n",
    "\n",
    "    cur_ep_ret = 0 # return in current episode\n",
    "    cur_ep_len = 0 # len of current episode\n",
    "    ep_rets = list() # returns of completed episodes in this segment\n",
    "    ep_lens = list() # lengths of ...\n",
    "\n",
    "    # Initialize history arrays\n",
    "    labels = np.array([label for _ in range(horizon)])\n",
    "    obs = np.array([ob for _ in range(horizon)])\n",
    "    rews = np.zeros(horizon, 'float32')\n",
    "    vpreds = np.zeros(horizon, 'float32')\n",
    "    news = np.zeros(horizon, 'int32')\n",
    "    acs = np.array([ac for _ in range(horizon)])\n",
    "    prevacs = acs.copy()\n",
    "\n",
    "    while True:\n",
    "        prevac = ac\n",
    "        ac, vpred = pi.act(stochastic, ob)\n",
    "        # Slight weirdness here because we need value function at time T\n",
    "        # before returning segment [0, T-1] so we get the correct\n",
    "        # terminal value\n",
    "        if t > 0 and t % horizon == 0:\n",
    "            m = {\"label\" : labels, \"ob\" : obs, \"rew\" : rews, \"vpred\" : vpreds, \"new\" : news,\n",
    "                    \"ac\" : acs, \"prevac\" : prevacs, \"nextvpred\": vpred * (1 - new),\n",
    "                    \"ep_rets\" : ep_rets, \"ep_lens\" : ep_lens}\n",
    "            # Be careful!!! if you change the downstream algorithm to aggregate\n",
    "            # several of these batches, then be sure to do a deepcopy\n",
    "            ep_rets = list()\n",
    "            ep_lens = list()\n",
    "        i = t % horizon\n",
    "        labels[i] = label\n",
    "        obs[i] = ob\n",
    "        vpreds[i] = vpred\n",
    "        news[i] = new\n",
    "        acs[i] = ac\n",
    "        prevacs[i] = prevac\n",
    "\n",
    "        ob, rew, new, _ = env.step(ac)\n",
    "        env.env.render()\n",
    "        label = env.env.labels()\n",
    "        rews[i] = rew\n",
    "\n",
    "        cur_ep_ret += rew\n",
    "        cur_ep_len += 1\n",
    "\n",
    "        if new:\n",
    "            ep_rets.append(cur_ep_ret)\n",
    "            ep_lens.append(cur_ep_len)\n",
    "            cur_ep_ret = 0\n",
    "            cur_ep_len = 0\n",
    "            ob = env.reset()\n",
    "            label = env.env.labels()\n",
    "        t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_net.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(env_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_reward = 0.0\n",
    "# num_rounds = 2\n",
    "# test_env =  gym.make(name, **names_and_args)\n",
    "# horizon = test_env.observation_space.K * num_rounds # generate around num_rounds draws\n",
    "# seg_gen = pposgd_simple_generalization.traj_segment_generator(\n",
    "#     policy_net, \n",
    "#     test_env, \n",
    "#     horizon, \n",
    "#     stochastic=True)\n",
    "# # call generator\n",
    "# results = seg_gen.__next__()\n",
    "# mean_reward = np.mean(results[\"ep_rets\"])\n",
    "# actions = results[\"ac\"]\n",
    "# labels = results[\"label\"]\n",
    "# mean_correct_actions = compute_correct_actions(labels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Train network over a number of repeats\n",
    "    import json\n",
    "    import os\n",
    "    repeats = 3\n",
    "    SAVE_FP = \"./tmp/models\"\n",
    "    os.makedirs(SAVE_FP, exist_ok=True)\n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "    results = defaultdict(lambda: defaultdict(lambda: dict()))\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    test_num_rounds = 100\n",
    "    for K in [10]:  # Fixed to 10\n",
    "        for potential in [.99]:\n",
    "            for adv_prob in tqdm(np.linspace(0, 1 , 20+1)):\n",
    "                names_and_args[\"K\"] = K\n",
    "                names_and_args[\"geo_high\"] = K-2\n",
    "                names_and_args[\"unif_high\"] = max(3, K-3)\n",
    "                names_and_args[\"potential\"] = potential\n",
    "                rewards = list()\n",
    "                test_rewards = list()\n",
    "                for rep in range(repeats):\n",
    "                    reset_session_and_graph()\n",
    "                    sess = U.single_threaded_session()\n",
    "                    with sess.as_default():\n",
    "                        erdos_env = gym.make(name, **names_and_args)\n",
    "                        pi, oldpi = make_policies(erdos_env.observation_space, \n",
    "                                                  erdos_env.action_space, \n",
    "                                                  policy_fn)                \n",
    "                        pi, info = train(erdos_env, pi, oldpi, \n",
    "                                         names_and_args, \n",
    "                                         num_timesteps=50000, \n",
    "                                         test_envs=list())  # Add test environnements maybe?\n",
    "                        rewards.append(info[\"rewards\"])\n",
    "\n",
    "                        # Test policy\n",
    "                        for test_env_adv_prob in tqdm(np.linspace(0, 1 , 20+1)):\n",
    "                            results[adv_prob][test_env_adv_prob][rep] = test_policy(test_num_rounds, pi, test_env)\n",
    "                        with open(\"results.json\", \"r\") as file:\n",
    "                            file.write(json.dumps(results))\n",
    "\n",
    "                        # Save model\n",
    "                        model_fp = \"{}model_K{}_potential{}_rep{}.ckpt\".format(\n",
    "                            SAVE_FP, K, potential, rep)\n",
    "                        save_session(model_fp)\n",
    "\n",
    "                # Save results\n",
    "                with open(SAVE_FP+\"rewards_K%02d_potential%f.p\"%(K, potential), \"wb\") as f:\n",
    "                    pickle.dump(rewards, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "print(os.listdir())\n",
    "with open(\"results.json\", \"r\") as file:\n",
    "    results = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['0.0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['0.0']['0.0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "keys = results.keys()\n",
    "data_1 = np.empty((len(keys), len(keys)))\n",
    "data_0 = np.empty((len(keys), len(keys)))\n",
    "for idx_i, key_i in enumerate(keys):\n",
    "    for idx_j, key_j in enumerate(keys):\n",
    "        li = list()\n",
    "        k = len(results[key_i][key_j])\n",
    "        for i in range(k):\n",
    "            li.append(results[key_i][key_j][str(i)])\n",
    "        li = np.array(li).mean(axis=0)\n",
    "        data_0[idx_i][idx_j] = li[0]\n",
    "        data_1[idx_i][idx_j] = li[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data_0)\n",
    "plt.show()\n",
    "plt.imshow(data_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
